{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project: Building a GNN-Based Recommender System\n",
    "\n",
    "**Course:** Recommender Systems\n",
    "\n",
    "**Estimated Time:** 4 Hours\n",
    "\n",
    "**Topic:** Link Prediction for Music Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project Goal\n",
    "\n",
    "The objective of this project is to build a recommender system that suggests new music artists to users. You will model the user-artist relationship as a **bipartite graph** and frame the recommendation task as a **link prediction problem**. You will implement a Graph Neural Network (GNN) to learn embeddings for users and artists, which will then be used to predict potential future interactions (i.e., which artists a user might like)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Core Technologies\n",
    "\n",
    "You will use the following key libraries:\n",
    "\n",
    "* **pandas:** For data loading and initial manipulation.\n",
    "\n",
    "* **NetworkX:** To construct the initial user-artist graph.\n",
    "\n",
    "* **PyTorch & PyTorch Geometric (PyG):** For defining the GNN model, preparing the data, and training.\n",
    "\n",
    "* **scikit-learn:** For evaluating the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The Dataset: Last.fm User-Artist Interactions\n",
    "\n",
    "We will use a dataset from the music streaming service **Last.fm**. It contains a list of users, the artists they have listened to, and the number of times they've played songs by that artist (a \"weight\"). This represents a rich set of implicit feedback.\n",
    "\n",
    "You can download the dataset here: [Last.fm Asia Social Network Dataset](https://files.grouplens.org/datasets/hetrec2011/hetrec2011-lastfm-2k.zip)\n",
    "\n",
    "The key file you'll need is `user_artists.dat`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Step-by-Step Implementation Guide\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 1: Environment Setup & Data Loading (Est. 20 mins)**\n",
    "\n",
    "Open the PowerShell Prompt as admin (in Windows) or the Terminal (in Mac and Linux), and strat by creating a conda environment:\n",
    "\n",
    "`conda create --name rec_gnn python=3.9`\n",
    "\n",
    "Install the demanded libraries:\n",
    "\n",
    "`pip install torch torch-geometric pandas networkx scikit-learn`\n",
    "\n",
    "Finally choose the kernel `rec_gnn` on VsCode to execute your code.\n",
    "\n",
    "**Load Data:** Use the `pandas` library to load the `user_artists.dat` file.\n",
    "  * Use the function `pd.read_csv()`, specifying the file path and setting the separator to `sep='\\t'`.\n",
    "  * Assign the column names: `['userID', 'artistID', 'listen_count']`.\n",
    "\n",
    "**Explore:** Print the head of the DataFrame and the number of unique users and artists to understand the data's scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports for the project\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Your code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"user_artists.dat\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"weight\": \"listen_count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>listen_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>51</td>\n",
       "      <td>13883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>11690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "      <td>11351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>10300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>6152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>5955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>4616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>4337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>4147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  artistID  listen_count\n",
       "0       2        51         13883\n",
       "1       2        52         11690\n",
       "2       2        53         11351\n",
       "3       2        54         10300\n",
       "4       2        55          8983\n",
       "5       2        56          6152\n",
       "6       2        57          5955\n",
       "7       2        58          4616\n",
       "8       2        59          4337\n",
       "9       2        60          4147"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 2: Building the Bipartite Graph with NetworkX (Est. 30 mins)**\n",
    "\n",
    "Now, convert the DataFrame of interactions into a single, undirected bipartite graph. A bipartite graph has two distinct sets of nodes, and edges only connect nodes from different sets (in our case, users to artists).\n",
    "\n",
    "1.  **Instantiate a Graph:** Create an empty graph object using `G = nx.Graph()`.\n",
    "2.  **Add Nodes with Attributes:** Iterate through the unique users and artists. For each one, you must add it to the graph with a special attribute to identify its type.\n",
    "    * For users, use `G.add_node(user_id, bipartite=0)`.\n",
    "    * For artists, use `G.add_node(artist_id, bipartite=1)`.\n",
    "3.  **Add Edges:** Iterate through each row of your DataFrame. For each row, create a connection between the user and the artist using `G.add_edge(row['userID'], row['artistID'])`. We will ignore the `listen_count` for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_id in df[\"userID\"].unique():\n",
    "    G.add_node(user_id, bipartite=0)\n",
    "\n",
    "for artist_id in df[\"artistID\"].unique():\n",
    "    G.add_node(artist_id, bipartite=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df.itertuples(index=False):\n",
    "    G.add_edge(int(row.userID), int(row.artistID))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 3: Preparing the Data with PyTorch Geometric (Est. 40 mins)**\n",
    "\n",
    "Next, you'll convert the NetworkX graph into a format that PyTorch Geometric can use. This involves creating a single `Data` object and then splitting its edges for training, validation, and testing.\n",
    "\n",
    "1.  **Map Node IDs:** Create a dictionary that maps every original `userID` and `artistID` to a unique integer from 0 to `num_nodes - 1`. This is required for the embedding layer.\n",
    "2.  **Create PyG Data Object:** Construct a `torch_geometric.data.Data` object. This will hold all the information about our graph.\n",
    "    * **Node Features (`x`):** We don't have initial features, so we'll use learnable embeddings instead. Create a tensor of node indices that the model will use to look up these embeddings. Use `torch.arange(num_nodes, dtype=torch.long)`.\n",
    "    * **Edge Index (`edge_index`):** Convert the list of graph edges into a tensor of shape `[2, num_edges]` where each column is an edge. Set its `dtype` to `torch.long`.\n",
    "3.  **Split Edges:** Use the modern `torch_geometric.transforms.RandomLinkSplit` transform to partition the edges. This is the standard method for link prediction tasks.\n",
    "    * Instantiate the transform: `transform = T.RandomLinkSplit(...)`.\n",
    "    * Inside the transform, set key parameters:\n",
    "        * `num_val=0.1` and `num_test=0.1` to hold out 10% of edges for validation and 10% for testing.\n",
    "        * `is_undirected=True` because our graph is undirected.\n",
    "        * `add_negative_train_samples=False` because we will perform negative sampling inside the training loop.\n",
    "        * `split_labels=True` to create the necessary `pos_edge_label_index` attributes.\n",
    "    * Apply it to your `data` object to get three new objects: `train_data, val_data, test_data = transform(data)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = pd.concat([df['userID'], df['artistID']]).unique()\n",
    "id_mapping = {original_id: new_id for new_id, original_id in enumerate(all_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17644"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.arange(len(id_mapping), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = df[[\"userID\",\"artistID\"]].copy()\n",
    "df_map[\"userID\"]= df_map[\"userID\"].map(id_mapping)\n",
    "df_map[\"artistID\"] = df_map[\"artistID\"].map(id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  artistID\n",
       "0       0        48\n",
       "1       0        49\n",
       "2       0        50\n",
       "3       0        51\n",
       "4       0        52\n",
       "5       0        53\n",
       "6       0        54\n",
       "7       0        55\n",
       "8       0        56\n",
       "9       0        57"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_map.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(\n",
    "    df_map[['userID', 'artistID']].values.T,  # transpose pour avoir [2, num_edges]\n",
    "    dtype=torch.long\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,  ...,  1891,  1891,  1891],\n",
       "        [   48,    49,    50,  ..., 17641, 17642, 17643]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(x=X, edge_index=edge_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,                 \n",
    "    num_test=0.1,               \n",
    "    is_undirected=True,  \n",
    "    add_negative_train_samples=False,\n",
    "    split_labels=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 4: Implementing the GNN Model (Est. 60 mins)**\n",
    "\n",
    "Define your GNN using PyTorch and PyG. The model will learn dense vector representations (embeddings) for all users and artists.\n",
    "\n",
    "1.  **Create the Class:** Define a class `GNNLinkPredictor` that inherits from `torch.nn.Module`.\n",
    "2.  **Define Layers in `__init__`:** The constructor should accept `num_nodes`, `embedding_dim`, `hidden_channels`, and `out_channels` as arguments.\n",
    "    * **Embedding Layer:** Create an `nn.Embedding(num_nodes, embedding_dim)` layer. This will store the learnable feature vector for every node in the graph.\n",
    "    * **Convolution Layers:** Define two `SAGEConv` layers. The first layer's `in_channels` must match your `embedding_dim`.\n",
    "3.  **Implement the Encoder:** Create an `encode` method that takes node indices (`x`) and the message-passing edges (`edge_index`) as input.\n",
    "    * First, pass the node indices `x` through your `self.embedding` layer to get the dense node features.\n",
    "    * Then, process these features through the two `SAGEConv` layers, using a ReLU activation function in between.\n",
    "4.  **Implement the Decoder:** Create a `decode` method that takes the final node embeddings (`z`) and a set of query edges (`edge_label_index`) as input.\n",
    "    * Use the `edge_label_index` to select the embeddings for the source and destination nodes of each edge.\n",
    "    * Calculate the dot product between the source and destination node embeddings for each edge. This dot product will be our link prediction score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GNNLinkPredictor(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_nodes, embedding_dim, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeding=nn.Embedding(num_nodes, embedding_dim)\n",
    "        \n",
    "        self.conv1=SAGEConv(embedding_dim,hidden_channels)\n",
    "        self.conv2=SAGEConv(hidden_channels,out_channels)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def encoder(self,x,edge_index):\n",
    "        x=self.embeding(x)\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def decoder(self,z,edge_index_label):\n",
    "        source = z[edge_index_label[0]]\n",
    "        destination = z[edge_index_label[1]]\n",
    "\n",
    "        product = (source * destination).sum(dim=1)\n",
    "\n",
    "        return product\n",
    "\n",
    "    def forward(self,x, edge_index,edge_label_index):\n",
    "        z=self.encoder(x,edge_index)\n",
    "        return self.decoder(z,edge_label_index)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 5: Training the Model (Est. 60 mins)**\n",
    "\n",
    "With the model and data ready, you will write a standard PyTorch training loop to optimize the model's parameters.\n",
    "\n",
    "1.  **Initialize:** Instantiate your `GNNLinkPredictor` model, an `Adam` optimizer, and the `BCEWithLogitsLoss` loss function, which is ideal for binary classification tasks like link prediction.\n",
    "2.  **Define the `train` function:** This function will perform one full training step. It should accept the `train_data` object as an argument.\n",
    "3.  **Inside the function:**\n",
    "    * Get the node indices (`x`), message-passing edges (`edge_index`), and positive supervision links (`pos_edge_label_index`) from the `train_data` object.\n",
    "    * **Negative Sampling:** For every positive link, we need a negative one. Use the `torch_geometric.utils.negative_sampling` function to generate a tensor of non-existent edges. Set `num_neg_samples` to be equal to the number of positive edges.\n",
    "    * Combine the positive and negative supervision links into a single `edge_label_index` tensor.\n",
    "    * Pass the node indices, message-passing edges, and combined supervision links to the model to get predictions.\n",
    "    * Create the ground-truth `edge_label` tensor: a tensor of 1s for the positive links and 0s for the negative links.\n",
    "    * Calculate the loss, backpropagate, and update the model weights with `optimizer.step()`.\n",
    "4.  **Run the Loop:** Call your `train(train_data)` function repeatedly in a `for` loop for a set number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GNNLinkPredictor(len(all_ids),20,10,1)\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (train_data):\n",
    "    edge_index = train_data.edge_index\n",
    "    pos_edge_label_index=train_data.pos_edge_label_index\n",
    "    dimention=len(pos_edge_label_index[1])\n",
    "    neg_sampling= negative_sampling(edge_index=edge_index,num_neg_samples=dimention)\n",
    "    edge_label_index=  torch.cat([pos_edge_label_index, neg_sampling], dim=1)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    edge_label=torch.cat([torch.ones(dimention),torch.zeros(dimention)])\n",
    "    pred = model(train_data.x, train_data.edge_index, edge_label_index)\n",
    "\n",
    "\n",
    "\n",
    "    loss=F.binary_cross_entropy_with_logits(pred,edge_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.7128\n",
      "Epoch 10/100, Loss: 0.6918\n",
      "Epoch 15/100, Loss: 0.6803\n",
      "Epoch 20/100, Loss: 0.6177\n",
      "Epoch 25/100, Loss: 0.5336\n",
      "Epoch 30/100, Loss: 0.5206\n",
      "Epoch 35/100, Loss: 0.5063\n",
      "Epoch 40/100, Loss: 0.5040\n",
      "Epoch 45/100, Loss: 0.5013\n",
      "Epoch 50/100, Loss: 0.4988\n",
      "Epoch 55/100, Loss: 0.4986\n",
      "Epoch 60/100, Loss: 0.4943\n",
      "Epoch 65/100, Loss: 0.4909\n",
      "Epoch 70/100, Loss: 0.4933\n",
      "Epoch 75/100, Loss: 0.4934\n",
      "Epoch 80/100, Loss: 0.4928\n",
      "Epoch 85/100, Loss: 0.4918\n",
      "Epoch 90/100, Loss: 0.4929\n",
      "Epoch 95/100, Loss: 0.4906\n",
      "Epoch 100/100, Loss: 0.4921\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    loss = train(train_data)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Step 6: Evaluation (Est. 20 mins)**\n",
    "\n",
    "Finally, evaluate your trained model's ability to distinguish between true and false links on the unseen test dataset. We will use the **AUC (Area Under the ROC Curve)** metric.\n",
    "\n",
    "1.  **Define the `test` function:** Create a function that accepts a data object (e.g., `val_data` or `test_data`). It should be decorated with `@torch.no_grad()` to disable gradient calculations.\n",
    "2.  **Inside the function:**\n",
    "    * Set the model to evaluation mode with `model.eval()`.\n",
    "    * Get the necessary tensors from the input `data` object: `x`, `edge_index` (for encoding), `pos_edge_label_index`, and `neg_edge_label_index` (for decoding).\n",
    "    * First, generate the final node embeddings by calling `model.encode(x, edge_index)`.\n",
    "    * Then, get the prediction scores for the test links by calling `model.decode(z, edge_label_index)`.\n",
    "    * Create the ground-truth labels (1s and 0s) for the positive and negative test links.\n",
    "    * Use `sklearn.metrics.roc_auc_score` to compute the AUC score between your predictions and the true labels.\n",
    "3.  **Run Evaluation:** Call your `test` function with the `test_data` object to get the final performance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(test_data):\n",
    "    model.eval()\n",
    "    \n",
    "    x=test_data.x\n",
    "    edge_index = test_data.edge_index\n",
    "    z = model.encoder(x, edge_index)\n",
    "    \n",
    "    pos_edge_label_index=test_data.pos_edge_label_index\n",
    "    neg_edge_label_index=test_data.neg_edge_label_index\n",
    "    edge_label_index = torch.cat([pos_edge_label_index, neg_edge_label_index], dim=1)\n",
    "    \n",
    "    pred=model.decoder(z, edge_label_index)\n",
    "\n",
    "    pos_labels = torch.ones(pos_edge_label_index.size(1))\n",
    "    neg_labels = torch.zeros(neg_edge_label_index.size(1))\n",
    "    edge_labels = torch.cat([pos_labels, neg_labels])\n",
    "\n",
    "    auc = roc_auc_score(edge_labels.cpu().numpy(), pred.cpu().numpy())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.7830\n"
     ]
    }
   ],
   "source": [
    "test_auc = test(test_data)\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Final Deliverables\n",
    "\n",
    "* A Python script (`.ipynb`) containing the full implementation of all steps.\n",
    "\n",
    "* A brief summary (in comments or a markdown cell) reporting your final test AUC score and a short reflection:\n",
    "\n",
    "    * Did the model perform better than random guessing?\n",
    "\n",
    "    * What is one way you could potentially improve the model if you had more time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final result\n",
    "\n",
    "Final AUC = 0.7830\n",
    "\n",
    "The model perform better than random guessing because it has 0.7830 of AUC that is better than 0.5 \n",
    "\n",
    "To improve the result we can expand the model to have a deeper model. We can also try to tune the hyperparameters or have more data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini_project_Recomended_systeme",
   "language": "python",
   "name": "mini_project_recomended_systeme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
